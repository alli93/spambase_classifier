\documentclass[11pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{graphicx}

\usepackage{hyperref} % for hyperlinks

\usepackage{listings} % for writing codes in latex
\usepackage{color}

\textheight25.5cm
\textwidth16.2cm
\voffset-2.4cm
\hoffset-1.8cm


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\renewcommand{\familydefault}{\sfdefault}


%------------------------------------------------------------
%		ORDER of SECTIONS
%
%		Title
%		Author's name and institutional affiliation
%		Author note
%		Abstract
%		Introduction
%		...
%		References
%		Appendices and supplemental materials


\begin{document}
\title{Spam e-mail classifiers using spambase dataset}

\author{
	Alexander Bj\"ornsson\\
	{\tt email1}
\and
	Sara Sabrina Zemlji\v c \\
	{\tt sara.zemljic@gmail.com}
}

\date{{\small School of Computer Science, Reykjavik University, Iceland}\\
\medskip
{\small \today}}
\maketitle

\begin{abstract}
We can all agree that the amount of spam we get every day by e-mail is almost limitless. The annoying spam messages are also getting more and more dangerous since they may contain viruses or other threats. Therefore it is of no surprise that spam filters have been studied quite intesively with various methods from machine learning.

We present our models for spam filtering on the dataset spambase. \textbf{**ADD which models and how impressive they are =)**}
\end{abstract}

\textbf{Keywords:} e-mail, spam, $k$ nearest-neighbors ($k$NN), Na\"ive Bayes (NB), artificial neural networks (ANN)

\section{Introduction}

Following the definition in~\cite[p.2]{Cormack-2006}, \textit{{\em spam}: unwanted communication intended to be deliv- ered to an indiscriminate target, directly or indirectly, notwithstanding measures to prevent its delivery; {\em spam filter}: an automated technique to identify spam for the purpose of preventing its delivery.}.

-- explain ham

-- a word about false positives and why we want to minimize them

--


Let us just list a few models that have been trained for e-mail spam detection...

-- a word about what has been studied on spam filters in general

-- a word about what has been studied on (our) spambase and where we found it (!!!)

-- maybe also why we decided for this dataset out of all others: the others would require way too much preprocesing with HTML and stuff like that which is not soooo relevant to this project

The rest of our paper is organized as follows. In the next section we introduce our dataset and what preprocesing we performed on it. In Section~\ref{sec-models} we discuss the three classifiers we trained and their evaluation is presented in Section~\ref{sec-evaluation}. We conclude the paper with final thoughts about the filters and present our code in Appendix.

\section{Dataset}
\label{sec-data}

The dataset we are using for this research is the {\em spambase}, a SPAM E-mail Database~\cite{spambase} donated by George Forman (gforman at nospam hpl.hp.com, 650-857-7835) from Hewlett-Packard Labs and was generated in June/July 1999. 
The spam e-mails in the collection include advertisements for products or web sites, make money fast schemes, chain letters, etc. The collection of non-spam e-mails in the database came from filed work and personal e-mails, therefore the dataset is very specific. For example words like 'george' or the code '650' are very strong indicators that an e-mail is not spam. 

There are 4601 instances out of which 1813 (about 39.4$\%$) are spam. Each instance is represented as a vector with 58 entries, so 57 + 1 columns, out of which
\begin{itemize}
  \item[--] the last one gives us the class information, it is either 1 (= spam) or 0 (= ham);
  \item[--] first 48 columns are continuous real attributes in the range $[0,100]$ of type {\tt word\_freq\_WORD} (e.g.\ {\tt word\_freq\_make}, {\tt word\_freq\_address}, {\tt word\_freq\_all}, etc.). 
  These attributes present the ration of the number of times the {\tt WORD} appears in the e-mail) over the total number of words in e-mail.  A "word" in this case is any 
string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.
  \item[--] next 6 columns are continuous real attributes in the range $[0,100]$ of type {\tt char\_freq\_CHAR} (e.g.\ {\tt char\_freq\_;}, {\tt char\_freq\_!}, {\tt char\_freq\_\$}), 
  which present the percentage of characters in the e-mail that match {\tt CHAR}.
  \item[--] last 3 attributes are continuous real in the range $[1,\ldots ]$ and are a bit special. They count occurences with capital letters. For example, {\tt capital\_run\_length\_average} counts average length of {\em uninterrupted sequences of capital letters}.
\end{itemize}

This dataset is preprocesed, which means the attributes were chosen this way to classify spam the best. It also has no missing values. The data is not available in the raw format, so it is imposible to experiment with other attributes that could be extracted from their e-mails. Despite that this dataset was used in various studies for testing different classifiers for recognizing spam e-mails.\\

At the beginning we split our data into train and test sets, using $20\%$ of the data for testing our models at the end. To ensure that we are always using the same train set we included the same {\tt random\_state} at all calculations. 
Our classes are in about $2:3$ ratio, so we checked that the train-test split has a similar ratio of spam and ham e-mails as well.

For some of our models we required normalized data, therefore we used \textbf{** which normalization?? **}


\bigskip
\textbf{Preprocessing??}\\
-- Are we doing any preprocesing??

\bigskip
\textbf{** anything else missing?? **}


\section{Models}
\label{sec-models}

We have decided to find a spam filter with our dataset with \textbf{** three **} different classifiers to determine which one gives us the best one. First we uses $k$ nearest-neighbour classifier ($k$NN shortly), then we studied Na\"ive Bayes (NB) methods on our dataset and finally we built a neural network (NN) classifier.

\textbf{** maybe make a combined one from all three? **}

\subsection{$k$ nearest-neighbour classifier}

-- PCA cross validation (for default $k$, which is 5??)

-- -- data is first split into train and test set, train set is then used for cross validation to determine which number of components would be the best to use; this is then used for (proper) PCA on the whole data set 

-- 

-- kNN is first trained on all 57 attributes and then compared with model obtained with "PCAed" data for the number of components that proved to be the best with cross-validation

-- then kNN is hypertuned for which $k$ would be the best: $k=13$

-- 


\subsection{Bayesian ...}

-- explain why multinomial (coz the distribution of attibutes is the closest to multinomial) 

\subsection{Neural networks}


\section{Evaluation of models}
\label{sec-evaluation}

bla bla introduction text :D

\subsection{kNN}

-- images for comparison

-- confusion matrix


\subsection{Bayesian ...}

\subsection{Neural networks}


\section{Conclusions}




\begin{thebibliography}{99}
%!! ALPHABETICALLY!!
\begin{small}
\bibitem{Metsis-2006}
  I.\ Androutsopoulos, V.\ Metsis, G.\ Paliouras,
  Spam Filtering with Naive Bayes -- Which Naive Bayes?,
  in: Proceedings of the Third Conference on Email and AntiSpam (2006).

\bibitem{Cormack-2006}
  G.\ V.\ Cormack, 
  Email Spam Filtering: A Systematic Review,
  Foundation and Trends in Information Retrieval (2006) 1(4) 335--455.

\bibitem{Idris-2011}
  I.\ Idris, 
  E-mail Spam Classification With Artificial Neural Network and Negative Selection Algorithm,
   International Journal of Computer Science \& Communication Networks 1(3) (2011) 227--231.

\bibitem{spambase}
  M.\ Hopkins, E.\ Reeber, G.\ Forman, J.\ Suermondt, 
  SPAM E-mail Database,
  Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304,
  \url{https://archive.ics.uci.edu/ml/datasets/spambase}.

\bibitem{Puniskis-2006}
  D.\ Puni\v skis, R.\ Laurutis, R.\ Dirmeikis,
  An artificial neural nets for spam eâ€“mail recognition,
  Elektronika ir Elektrotechnika (Electronics and Electrical Engineering) 5(69) (2006) 73--76.

\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.


\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.

 
\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.


        
        
\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.

\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.


\bibitem{autor-year}
  autors, 
  title-of-article,
  journal issue-nr (year) pages--pages.
\end{small}
\end{thebibliography}

\section*{Appendix: code}
STRUCTURE THE CODE into smaller segments, we will only have main training parts here (no prints, no confusion matrices codes etc). Only main models and their preprocesing, for results we will just analyze them with words
\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsClassifier
from  sklearn.model_selection  import  train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from matplotlib.ticker import FuncFormatter
from sklearn.model_selection import KFold
from  sklearn.model_selection  import  train_test_split
from sklearn.preprocessing import Normalizer
from sklearn.metrics import precision_score

from sklearn.metrics import confusion_matrix

"""
loading and preprocesing the data
"""

file = open("spambase.data")
data = np.loadtxt(file,delimiter=",")

X = data[:, 0:57]
y = data[:, 57]

dataframe = pd.DataFrame(data=X)

#apply normalization function to every attribute
dataframe_norm = dataframe.apply(lambda x: (x - np.mean(x)) / np.std(x))

"""
kNN model
"""
# explained_variance_ratio_ for n_components
EVC = []
for attribute in dataframe_norm:
    pca = PCA(n_components=attribute)
    pca.fit(dataframe_norm)
    EVC.append(pca.explained_variance_ratio_.sum())

# n_components hyperparameter tuning using KFold cross-validation
splits = 10
kf = KFold(n_splits = splits)
n_components_kFold_train = np.zeros(n_attributes-1)
n_components_kFold_test = np.zeros(n_attributes-1)
kFold_train_precision = np.zeros(n_attributes-1)
kFold_test_precision = np.zeros(n_attributes-1)
for i in range(1, n_attributes):
    X = pca(i, dataframe)
    # Using the same random_state ensures we do not contaminate our training data with our test data
    X_train ,  X_test ,  y_train ,  y_test = train_test_split(X,  y,  test_size=0.20 ,  random_state=42)
    for cv_train_index, cv_test_index in kf.split(X_train):
        X_cv_train = [X_train_1[i] for i in cv_train_index]
        y_cv_train = [y_train_1[i] for i in cv_train_index]
        X_cv_test = [X_train_1[i] for i in cv_test_index]
        y_cv_test = [y_train_1[i] for i in cv_test_index]
        # k = 5 by defult
        kNN = KNeighborsClassifier()
        kNN.fit(X_cv_train, y_cv_train) 
        kFold_train_precision[i-1] += precision_score(y_cv_train, kNN.predict(X_cv_train), average = 'micro') / splits
        kFold_test_precision[i-1]  += precision_score(y_cv_test, kNN.predict(X_cv_test), average = 'micro') / splits

# n_neighbors cross-validation
n_components = 14 
X = pca(n_components, dataframe_norm)
X_train ,  X_test ,  y_train ,  y_test = train_test_split(X,  y,  test_size=0.20 ,  random_state=42)

k_tests = list(range(1, 100))
k_precision_train = np.zeros(len(k_tests))
k_precision_test = np.zeros(len(k_tests))

kf = KFold(n_splits = splits)
for cv_train_index, cv_test_index in kf.split(X_train):
        X_cv_train = [X_train[i] for i in cv_train_index]
        y_cv_train = [y_train[i] for i in cv_train_index]
        X_cv_test = [X_train[i] for i in cv_test_index]
        y_cv_test = [y_train[i] for i in cv_test_index]
        i=0
        for test in k_tests:
            kNN = KNeighborsClassifier(n_neighbors = test)
            kNN.fit(X_cv_train, y_cv_train) 
             k_precision_train [i-1] += precision_score(y_cv_train, kNN.predict(X_cv_train), average = 'micro') / splits
            k_precision_test [i-1]  += precision_score(y_cv_test, kNN.predict(X_cv_test), average = 'micro') / splits
            i += 1

X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm,  y, test_size=0.20 , random_state=42)

kNN = KNeighborsClassifier(n_neighbors = k)
kNN.fit(X_train, y_train)

kNN_precision_train_pre = precision_score(y_train, kNN.predict(X_train), average = 'micro')
kNN_precision_test_pre = precision_score(y_test, kNN.predict(X_test), average = 'micro')

k = 4 
X = pca(n_components, dataframe_norm)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20 , random_state=42)

kNN_precision_train_post = precision_score(y_train, kNN.predict(X_train), average = 'micro')
kNN_precision_test_post = precision_score(y_test, kNN.predict(X_test), average = 'micro')



kNN_precision_train_post = precision_score(y_train, kNN.predict(X_train), average = 'micro')
precision_test_post = precision_score(y_test, kNN.predict(X_test), average = 'micro')


"""
NB model
"""

# Different preprocessings
dataframe = pd.DataFrame(data=X)
dataframe_norm = dataframe.apply(lambda x: (x - np.mean(x)) / np.std(x))
min_max_scaler = preprocessing.MinMaxScaler()
dataframe_norm_scaled = min_max_scaler.fit_transform(dataframe_norm)
dataframe_norm_upscaled = dataframe_norm.apply(lambda x: (x + abs(min(x))))

# No preprocessing of the data
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe,  y,  test_size=0.20 ,  random_state=42)
mNB = GaussianNB()
mNB.fit(X_train, y_train)
precision_train = precision_score(y_train, mNB.predict(X_train), average = 'micro')
precision_test = precision_score(y_test, mNB.predict(X_test), average = 'micro')

# Upscaled normalized data with no negative attributes
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_upscaled,  y,  test_size=0.20 ,  random_state=42)
mNB_upscaled = GaussianNB()
mNB_upscaled.fit(X_train, y_train)
norm_upscaled_precision_train = precision_score(y_train, mNB_upscaled.predict(X_train), average = 'micro')
norm_upscaled_precision_test = precision_score(y_test, mNB_upscaled.predict(X_test), average = 'micro')


# Scaled normalized data 
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_scaled,  y,  test_size=0.20 ,  random_state=42)
mNB_scaled = GaussianNB()
mNB_scaled.fit(X_train, y_train)
norm_scaled_precision_train = precision_score(y_train, mNB_scaled.predict(X_train), average = 'micro')
norm_scaled_precision_test = precision_score(y_test, mNB_scaled.predict(X_test), average = 'micro')


# kFold cross-validation for alpha
splits = 10
kf = KFold(n_splits = splits)

X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_scaled,  y,  test_size=0.20 ,  random_state=42)

alpha_tests = np.arange(0, 10.0, 0.01)
alpha_cv_train = np.zeros(len(alpha_tests))
alpha_cv_test = np.zeros(len(alpha_tests))
for cv_train_index, cv_test_index in kf.split(X_train):
    X_cv_train = [X_train[i] for i in cv_train_index]
    y_cv_train = [y_train[i] for i in cv_train_index]
    X_cv_test = [X_train[i] for i in cv_test_index]
    y_cv_test = [y_train[i] for i in cv_test_index]
    i = 0
    for test in alpha_tests:
        mNB_cv = MultinomialNB(alpha=test)
        mNB_cv.fit(X_cv_train, y_cv_train)
        alpha_cv_train[i] += precision_score(y_cv_train, mNB_cv.predict(X_cv_train), average = 'micro') / splits
        alpha_cv_test[i]  += precision_score(y_cv_test, mNB_cv.predict(X_cv_test), average = 'micro') / splits
        i += 1

\end{lstlisting}

\end{document}

%%%%%%%%%%%%%%%%%%%%

\begin{lstlisting}
\end{lstlisting}

\end{itemize}




