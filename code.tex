\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsClassifier
from  sklearn.model_selection  import  train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from matplotlib.ticker import FuncFormatter
from sklearn.model_selection import KFold
from  sklearn.model_selection  import  train_test_split
from sklearn.preprocessing import Normalizer
from sklearn.metrics import precision_score

from sklearn.metrics import confusion_matrix

"""
loading and preprocesing the data
"""

file = open("spambase.data")
data = np.loadtxt(file,delimiter=",")

X = data[:, 0:57]
y = data[:, 57]

dataframe = pd.DataFrame(data=X)

#apply normalization function to every attribute
dataframe_norm = dataframe.apply(lambda x: (x - np.mean(x)) / np.std(x))

"""
kNN model
"""
# explained_variance_ratio_ for n_components
EVC = []
for attribute in dataframe_norm:
    pca = PCA(n_components=attribute)
    pca.fit(dataframe_norm)
    EVC.append(pca.explained_variance_ratio_.sum())

# n_components hyperparameter tuning using KFold cross-validation
splits = 10
kf = KFold(n_splits = splits)
n_components_kFold_train = np.zeros(n_attributes-1)
n_components_kFold_test = np.zeros(n_attributes-1)
kFold_train_precision = np.zeros(n_attributes-1)
kFold_test_precision = np.zeros(n_attributes-1)
for i in range(1, n_attributes):
    X = pca(i, dataframe)
    # Using the same random_state ensures we do not contaminate our training data with our test data
    X_train ,  X_test ,  y_train ,  y_test = train_test_split(X,  y,  test_size=0.20 ,  random_state=42)
    for cv_train_index, cv_test_index in kf.split(X_train):
        X_cv_train = [X_train_1[i] for i in cv_train_index]
        y_cv_train = [y_train_1[i] for i in cv_train_index]
        X_cv_test = [X_train_1[i] for i in cv_test_index]
        y_cv_test = [y_train_1[i] for i in cv_test_index]
        # k = 5 by defult
        kNN = KNeighborsClassifier()
        kNN.fit(X_cv_train, y_cv_train) 
        kFold_train_precision[i-1] += precision_score(y_cv_train, kNN.predict(X_cv_train), average = 'micro') / splits
        kFold_test_precision[i-1]  += precision_score(y_cv_test, kNN.predict(X_cv_test), average = 'micro') / splits

# n_neighbors cross-validation
n_components = 14 
X = pca(n_components, dataframe_norm)
X_train ,  X_test ,  y_train ,  y_test = train_test_split(X,  y,  test_size=0.20 ,  random_state=42)

k_tests = list(range(1, 100))
k_precision_train = np.zeros(len(k_tests))
k_precision_test = np.zeros(len(k_tests))

kf = KFold(n_splits = splits)
for cv_train_index, cv_test_index in kf.split(X_train):
        X_cv_train = [X_train[i] for i in cv_train_index]
        y_cv_train = [y_train[i] for i in cv_train_index]
        X_cv_test = [X_train[i] for i in cv_test_index]
        y_cv_test = [y_train[i] for i in cv_test_index]
        i=0
        for test in k_tests:
            kNN = KNeighborsClassifier(n_neighbors = test)
            kNN.fit(X_cv_train, y_cv_train) 
             k_precision_train [i-1] += precision_score(y_cv_train, kNN.predict(X_cv_train), average = 'micro') / splits
            k_precision_test [i-1]  += precision_score(y_cv_test, kNN.predict(X_cv_test), average = 'micro') / splits
            i += 1

X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm,  y, test_size=0.20 , random_state=42)

kNN = KNeighborsClassifier(n_neighbors = k)
kNN.fit(X_train, y_train)

kNN_precision_train_pre = precision_score(y_train, kNN.predict(X_train), average = 'micro')
kNN_precision_test_pre = precision_score(y_test, kNN.predict(X_test), average = 'micro')

k = 4 
X = pca(n_components, dataframe_norm)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20 , random_state=42)

kNN_precision_train_post = precision_score(y_train, kNN.predict(X_train), average = 'micro')
kNN_precision_test_post = precision_score(y_test, kNN.predict(X_test), average = 'micro')



kNN_precision_train_post = precision_score(y_train, kNN.predict(X_train), average = 'micro')
precision_test_post = precision_score(y_test, kNN.predict(X_test), average = 'micro')


"""
NB model
"""

# Different preprocessings
dataframe = pd.DataFrame(data=X)
dataframe_norm = dataframe.apply(lambda x: (x - np.mean(x)) / np.std(x))
min_max_scaler = preprocessing.MinMaxScaler()
dataframe_norm_scaled = min_max_scaler.fit_transform(dataframe_norm)
dataframe_norm_upscaled = dataframe_norm.apply(lambda x: (x + abs(min(x))))

# No preprocessing of the data
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe,  y,  test_size=0.20 ,  random_state=42)
mNB = GaussianNB()
mNB.fit(X_train, y_train)
precision_train = precision_score(y_train, mNB.predict(X_train), average = 'micro')
precision_test = precision_score(y_test, mNB.predict(X_test), average = 'micro')

# Upscaled normalized data with no negative attributes
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_upscaled,  y,  test_size=0.20 ,  random_state=42)
mNB_upscaled = GaussianNB()
mNB_upscaled.fit(X_train, y_train)
norm_upscaled_precision_train = precision_score(y_train, mNB_upscaled.predict(X_train), average = 'micro')
norm_upscaled_precision_test = precision_score(y_test, mNB_upscaled.predict(X_test), average = 'micro')


# Scaled normalized data 
X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_scaled,  y,  test_size=0.20 ,  random_state=42)
mNB_scaled = GaussianNB()
mNB_scaled.fit(X_train, y_train)
norm_scaled_precision_train = precision_score(y_train, mNB_scaled.predict(X_train), average = 'micro')
norm_scaled_precision_test = precision_score(y_test, mNB_scaled.predict(X_test), average = 'micro')


# kFold cross-validation for alpha
splits = 10
kf = KFold(n_splits = splits)

X_train ,  X_test ,  y_train ,  y_test = train_test_split(dataframe_norm_scaled,  y,  test_size=0.20 ,  random_state=42)

alpha_tests = np.arange(0, 10.0, 0.01)
alpha_cv_train = np.zeros(len(alpha_tests))
alpha_cv_test = np.zeros(len(alpha_tests))
for cv_train_index, cv_test_index in kf.split(X_train):
    X_cv_train = [X_train[i] for i in cv_train_index]
    y_cv_train = [y_train[i] for i in cv_train_index]
    X_cv_test = [X_train[i] for i in cv_test_index]
    y_cv_test = [y_train[i] for i in cv_test_index]
    i = 0
    for test in alpha_tests:
        mNB_cv = MultinomialNB(alpha=test)
        mNB_cv.fit(X_cv_train, y_cv_train)
        alpha_cv_train[i] += precision_score(y_cv_train, mNB_cv.predict(X_cv_train), average = 'micro') / splits
        alpha_cv_test[i]  += precision_score(y_cv_test, mNB_cv.predict(X_cv_test), average = 'micro') / splits
        i += 1

\end{lstlisting}
